{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moneyScrapper(url):\n",
    "    '''\n",
    "    moneyScrapper -> function scraps data from the money control website\n",
    "\n",
    "    Arguments\n",
    "        url - input url to the webpage to be scraped\n",
    "\n",
    "    Variables\n",
    "        page -> GET request response\n",
    "        soup -> BeautifulSoup object to get the HTML structure of the webpage\n",
    "\n",
    "        heading -> heading of the extracted article\n",
    "        time_stamp -> data and time of the posting of the article\n",
    "        image_source -> url of the image posted along the article\n",
    "        text -> extracted text of the article, from the <p> tags\n",
    "        article_content -> string of all text from different <p> tags added together\n",
    "\n",
    "    Return\n",
    "        a list containing heading, time_stamp, image_source, article_content, url of webpage\n",
    "    '''\n",
    "\n",
    "\n",
    "    print('[NOTE] Scraping data from Money Control........')\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    \n",
    "    #article heading \n",
    "    heading = soup.h1.getText()\n",
    "    \n",
    "    #article posting data and time\n",
    "    time_stamp = re.sub('/', ',', soup.find('div', {'class':'article_schedule'}).get_text())\n",
    "    \n",
    "    #article image url\n",
    "    image_source = soup.find('div', {'class':'article_image'})\n",
    "    image_source = image_source.find('img').get('data-src')\n",
    "    \n",
    "    #article content and text\n",
    "    article_content = ''\n",
    "    texts = soup.find_all('p')\n",
    "    texts = texts[67:]\n",
    "    for text in texts:\n",
    "        article_content = article_content + (text.getText())\n",
    "\n",
    "    # list of extracted information\n",
    "    return [heading, time_stamp, image_source, article_content, url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def economicScrapper(url):\n",
    "    '''\n",
    "    economicScrapper -> function scraps data from the economic times website\n",
    "\n",
    "    Arguments\n",
    "        url - input url to the webpage to be scraped\n",
    "\n",
    "    Variables\n",
    "        page -> GET request response\n",
    "        soup -> BeautifulSoup object to get the HTML structure of the webpage\n",
    "\n",
    "        heading -> heading of the extracted article\n",
    "        time_stamp -> data and time of the posting of the article\n",
    "        image_source -> url of the image posted along the article\n",
    "        text -> extracted text of the article, from the <p> tags\n",
    "        article_content -> string of all text from different <p> tags added together\n",
    "\n",
    "    Return\n",
    "        a list containing heading, time_stamp, image_source, article_content, url of webpage\n",
    "    '''\n",
    "\n",
    "    print('[NOTE] Scrapping data from Economic times......')\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "\n",
    "    #article heading\n",
    "    heading = soup.h1.getText()\n",
    "\n",
    "    #article posting date and time\n",
    "    time_stamp = re.split(':', soup.find('time').get_text(), 1)\n",
    "    time_stamp = time_stamp[1]\n",
    "    time_stamp = re.sub('Oct', 'October', time_stamp)       #time_stamp modified in accordance with money control website time_stamp format\n",
    "    \n",
    "    #article image\n",
    "    image_source = '_'                  #economics times articles had no images associated with them\n",
    "    \n",
    "    #article content and text\n",
    "    article_content = soup.find('div', {'class':'artText'})\n",
    "    article_content = article_content.get_text()\n",
    "\n",
    "    #list of extracted data\n",
    "    return [heading, time_stamp, image_source, article_content, url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(urls_df):\n",
    "    '''\n",
    "    scraper -> function scrapes the data from a given list of urls\n",
    "\n",
    "    Arguments\n",
    "        urls_df -> list of urls of websites to be extracted\n",
    "\n",
    "    Variables\n",
    "        money_control -> return a regular expression object for the sequence moneycontrol\n",
    "        economic_times -> return a regular expression object for the sequence economictimes\n",
    "\n",
    "        output_list -> output list of extracted data from the website\n",
    "    \n",
    "    Return\n",
    "        returns the output_list\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    money_control = re.compile(r'moneycontrol')\n",
    "    economic_times = re.compile(r'economictimes')\n",
    "\n",
    "    output_list = []\n",
    "\n",
    "    \n",
    "    for url in urls_df:\n",
    "\n",
    "        #check if the given url is from money control\n",
    "        if money_control.search(url):\n",
    "            output_list.append(moneyScrapper(url))\n",
    "\n",
    "        #check if the given url is from money control\n",
    "        else:\n",
    "            output_list.append(economicScrapper(url))\n",
    "\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] Scraping data from Money Control........\n",
      "[NOTE] Scraping data from Money Control........\n",
      "[NOTE] Scraping data from Money Control........\n",
      "[NOTE] Scraping data from Money Control........\n",
      "[NOTE] Scraping data from Money Control........\n",
      "[NOTE] Scrapping data from Economic times......\n",
      "[NOTE] Scrapping data from Economic times......\n",
      "[NOTE] Scrapping data from Economic times......\n",
      "[NOTE] Scrapping data from Economic times......\n",
      "[NOTE] Scrapping data from Economic times......\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # read the csv file with the given urls\n",
    "    urls_df = pd.read_csv('/home/karandeep/Downloads/web_scraping/urls.csv')\n",
    "\n",
    "    #convert the dataframe to list\n",
    "    urls_df = list(urls_df['web_urls'])\n",
    "\n",
    "    #extract the data in the form of a list\n",
    "    output_list = scraper(urls_df)\n",
    "\n",
    "    #create a dataframe of the extraceted data\n",
    "    output_data = pd.DataFrame(output_list, columns=['heading', 'time_stamp', 'image_urls', 'article_content', 'source_url'])\n",
    "\n",
    "    # save the extracted data to a csv file\n",
    "    output_data.to_csv('scraped_data.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb2cca092c916f40b972c6d12e0db3dab0b763a01e096fddb029601b63c7e9f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
